# -*- coding: utf-8 -*-
"""Drugs4Covid - Solr Corpus and Language Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1awDkPtxY8YrDJMrnsOyFO-QLnM9iy86Z
"""



# Commented out IPython magic to ensure Python compatibility.

import pandas as pd
import numpy as np
import re
import nltk
#import matplotlib.pyplot as plt
import pysolr

pd.options.display.max_colwidth = 200
# %matplotlib inline

!pip install pysolr==3.9.0
#!mkdir data

"""Function to retrieve all documents"""

def getAllDocuments(solr, start, limit_rows):
    results = solr.search('*:*', **{
    'start': str(start),
    'rows': limit_rows,
    })
    return results

"""Connect to database"""

path_to_data= '/home/s730/PROJECT/data/'


# big timeout for big queries
# Carlos full documents: https://librairy.linkeddata.es/solr/covid/
solr = pysolr.Solr('https://librairy.linkeddata.es/solr/covid/',always_commit=True, timeout=1000)

"""get documents"""



len(results)

"""### CreateCorpus


"""



text_data = []
file_count = 0


max_files=10000
start_point=0



results = getAllDocuments(solr, start_point , 10000)
start_point=start_point+max_files
#should be 34000

while len(results) == max_files:
  print('start_point:'+str(start_point))
  for result in results:
    try:            
      field_text = result["txt_t"]

      
      
      #doc = nlp(field_texto)

      #for sent in doc.sents:
      # le = len(sent.text)
        #if le > 4:
        # print(sent.text)
      text_data.append(field_text)
      if len(text_data) == 10_000:
        # once we git the 10K mark, save to file
        with open(f'{path_to_data}text_{file_count}.txt', 'w', encoding='utf-8') as fp:
          fp.write('\n'.join(text_data))
        text_data = []
        file_count += 1

    except:
      continue

  results = getAllDocuments(solr, start_point , 10000)
  start_point=start_point+max_files
#should be 34000

# after saving in 10K chunks, 
print(len(text_data))
with open(f'{path_to_data}text_{file_count}.txt', 'w', encoding='utf-8') as fp:
  fp.write('\n'.join(text_data))

from tqdm.auto import tqdm

text_data = []
file_count = 0

for sample in tqdm(dataset['train']):
    sample = sample['text'].replace('\n', '').replace('"', '')
    text_data.append(sample)
    if len(text_data) == 10_000:
        # once we git the 10K mark, save to file
        with open(f'data/text/text_{file_count}.txt', 'w', encoding='utf-8') as fp:
            fp.write('\n'.join(text_data))
        text_data = []
        file_count += 1
# after saving in 10K chunks, we will have ~2082 leftover samples, we save those now too
with open(f'data/text/text_{file_count}.txt', 'w', encoding='utf-8') as fp:
    fp.write('\n'.join(text_data))

corpus = np.array(corpus)
